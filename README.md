# GALOIS Project
### University of Padova â€“ Graph Database course

---

## Group Information
**Group Name:** GALILEO
**Members:**
- Giorgia Amato â€“ [giorgia.amato@studenti.unipd.it]  
- [Alessio Demo] â€“ [alessio.demo@studenti.unipd.it]  
- [Francesco Pivotto] â€“ [francesco.pivotto.1@studenti.unipd.it]   



---

##  Project Description
We are a student group exploring how **Large Language Models (LLMs)** can collaborate with **databases**.  
Our focus is on **GALOIS**, a framework that integrates LLMs into query execution pipelines.

**Objective:**
- Understand how this approach can improve **data retrieval**.  
- Analyze its **strengths and limitations**.  
- Implement a **practical use case** to demonstrate and test these ideas.

---

## Structure of the Repository

**The Repository is organized by functional scopes, mirroring the development phases and the required tasks.**

**The goal is to clearly separate the setup, the baselines, the core system, and the verification tools (results and testing).**

### Main Files

| File/Folder | Description                                                                                                                     |
| :--- |:--------------------------------------------------------------------------------------------------------------------------------|
| **`README.md`** | **General Project Report.** Describes the architecture, scheduling, summary of results, and execution instructions.             |
| **`requirements.txt`** | List of all necessary Python dependencies to replicate the environment (e.g., `sqlglot`, `langchain`, `ibm-watsonx`, `pandas`). |
| **`.gitignore`** | Standard file to ignore generated or temporary files.                                                                           |

***

### 1.  `setup-environment/` (Scope: Setup and Data)

Contains everything needed for the initial configuration, database connection, and management of test data.

| Subfolder/File         | Purpose |
|:-----------------------| :--- |
| **`config/`**          | Configuration scripts for the environment (e.g., environment variables, LLM API keys). |
| **`data/`**            | Contains the input data for the project. |
| â”œâ”€â”€ `input_dbs/`       | The provided SQL database files (e.g., `.db` files, PostgreSQL setup script). |
| â””â”€â”€ `queries/`         | Set of the provided **test queries**, in both SQL and Natural Language (NL) format. |
| **`report.pdf`** | **Task 1 Deliverable.** The final PDF to be uploaded to Crane (Setup and code structure). |

***

### 2.  `baselines/` (Scope: Reference Systems)

Dedicated to the implementation and execution of the reference systems required for comparative evaluation.

| Subfolder/File         | Purpose                                                                                |
|:-----------------------|:---------------------------------------------------------------------------------------|
| **`src/`**             | Source code for the Baselines.                                                         |
| **`results/`**         | Outputs generated from the baseline executions (NL & SQL).                             |
| **`report.pdf`** | **Task 2 Deliverable.** The final PDF to be uploaded to Crane (Baselines and metrics). |

***

### 3.  `galois-core-system/` (Scope: LLM Core System)

Contains the implementation of the LLM system based on the **Galois** paper, which is the core of the project.

| Subfolder/File         | Purpose |
|:-----------------------| :--- |
| **`src/`**             | Source code for the Galois system. |
| **`results/`**         | Outputs generated by the final system. |
| **`report.pdf`** | **Task 3 Deliverable.** The final PDF to be uploaded to Crane (Galois System and conclusions). |

***

### 4. âœ… `verification-test-tools/` (Scope: Testing and Verification)

Tools to ensure code correctness and verification.

| Subfolder/File | Purpose |
| :--- | :--- |
| **`tests/`** | Modules for unit tests and integration tests, used to validate individual components. |

---

## Architecture of the system:
### ðŸ›  Dependency Stack and Key Requirements

This project relies on a Python **dependency stack** designed to ensure robustness, efficient data analysis, and flexible configuration. All libraries required for execution (excluding development-only dependencies) are listed in `requirements.txt`.

### **Core Frameworks & Data Analysis (DataFrames)**

* **DataFrames/Analysis:** **`pandas`** and **`numpy`** are used in our project for the efficient manipulation and analysis of tabular data structures (DataFrames) and complex numerical calculations.

---

### **Database and SQL**

For connectivity, analysis, and management of structured data:

* **Database In-Process:** **`duckdb`** is used as a high-performance analytical database, ideal for rapid processing of data volumes directly within the application process.
* **SQL Parsing:** **`sqlglot`** is employed for the manipulation, translation, and abstract analysis of SQL queries.

---

### **LLM orchestrator**

Our system leverages LangChain to manage interactions with Google Gemini AI models through the **`langchain-google-genai`** package.
The reason why we choose LangChain is the modularity of this framework for handling prompts and conversational pipelines, while the Gemini integration enables access to Googleâ€™s advanced language understanding and generation capabilities.
In summary this setup is flexible and scalable in order to establish a communication between our system and LLM.

---

### **Config, Secrets, and Data Validation (Pydantic)**

Robust configuration management and input data validation are ensured via:

* **Configuration/Secrets:** **`python-dotenv`** loads environment variables (including secrets) from the `.env` file.
* **Data Validation:** We use  **`pydantic`** define structured configuration models, validate the data, and enforce type safety.
  * **Type validation**: ensures all config fields have the correct type.
  * **Data parsing / coercion**: converts types automatically when possible.
  * **Structured, nested models**: makes the configuration predictable and IDE-friendly.
  * **Error reporting**: immediately raises errors if the YAML is malformed or missing fields.
  * **Environment variable integration via BaseSettings**.


* **Config Parsing (YAML):** **`pyyaml`** handles reading and writing configuration files in YAML format.

---

### **Templating**

For the dynamic generation of code, reports, or structured output:

* Templates:We use  **`jinja2`** that allows separate presentation logic from data: indeed we generate dynamic text files (like HTML, XML, JSON) or prompt statements by combining static content with data from our system.

---

### **Logging and Utilities**

For monitoring and diagnostics:

* **Logging:** **`loguru`** is the chosen logging library for its simplicity, structured output, improving system log readability.
* **A folder named ** **`.log` ** contains all the logs generated during the running time. 

---

### **Development and Testing (Tests)**

These libraries are required only for the development environment and for running the project's test suite:

* **Test Runner:** **`pytest`** is the standard testing framework used to execute all tests.
* **Code Coverage:** **`coverage`** and **`pytest-cov`** measure the percentage of source code covered by automated tests.
* **Mocking:** **`pytest-mock`** facilitates the creation of mock objects and stubs to isolate external dependencies during unit tests.

---

## How to run the system:
For running the system the only script that you need is **`setup_project.py`** that is located in the root folder.
In detail what **`setup_project.py`** does is the following:
* Defines the main paths of the project (query results folder, ground truth folder exc...)
* Creates an virtual environment for python 
* Install the needed python dependencies from **requirements.txt** for the system
* Tests the database connection
* Executes queries for a set of datasets and saves results in JSON.
* Evaluates the results against the ground truth.
* Prints messages to track progress.


**RUN THE SYSTEM**: To run the system, execute **`python setup_project.py`** from the root directory (**`/galileo_proj/`**).

You can specify which datasets to process directly from the command line or via YAML configuration:
REMEMBER: THE COMMAND LINE HAS THE PRIORITY
* **To process all datasets**: **`python setup_project.py ALL`** 
* **To process one or more specific datasets**: **`python3 setup_project.py GEO MOVIES FLIGHT-4`** 
* **Alternative via Configuration File**: If no command-line arguments are given, the script will fall back to the YAML configuration in which you can define the datasets in **`config/config.yaml`** changing the selected datasets under the **`database`** attribute.

Furthermore, if necessary, it's possible to execute individual scripts:
* **Evaluation queries**: from the following directory: `/galileo_proj/src/utils/` run: **`python3  galileo_eval.py [-h] --ground GROUND --submissions SUBMISSIONS [--datasets [DATASETS ...]] [--cell-metric {exact,similarity}] [--tuple-metric {constraint,similarity}] [--format {table,csv,json,tex}]
                      [--latex-caption LATEX_CAPTION] [--latex-label LATEX_LABEL] [--latex-booktabs] [--overall] [--jobs JOBS] [--jobs-queries JOBS_QUERIES]`** .
* **Ground Truth generation**: from the following directory: `/galileo_proj/src/utils/` run:  **`python3 build_ground_truth.py [-h] --data-root DATA_ROOT --ground-root GROUND_ROOT [--datasets [DATASETS ...]] [--schema-name SCHEMA_NAME]
build_ground_truth.py: error: the following arguments are required: --data-root, --ground-root`**.
* **Avg. expected cells metric**: For calculate this metric you need to locate in the root  folder `/galileo_proj/` and run: **` python3 -m src.db.avg_cells_metric`**.
* **EXPLAIN / ANALYZE plans generation in .txt and .json format:** from the root folder `/galileo_proj/` run: **`python3 -m src.db.run_explain_plans all/<DATASETNAME>`** -> you can type ' all ' or ' ALL ' and the command works anyway, additionally you can specify a single dataset but you need to specify it in uppercase e.g. MOVIES.

---

## Implementation Design 

###  Database Preparation and Connection

### 1. Opening the Connection (db.connection.py)

The `db.connection.py` script is responsible for opening and managing the connection to the specific DuckDB database for the dataset being used.

**Operational Details:**

* **Path Calculation:** Calculates the exact path of the database file: `../data/<dataset_name>/<dataset_name>.duckdb`.
* **Existence Check:** Performs a check to ensure the database file exists at the specified path.
* **Connection Creation:** Creates and returns a DuckDB connection object (`duckdb.connect()`).
* **Feedback:** Prints a confirmation message indicating that the connection has been successfully established.

---

### 2. Ingestion and Setup (duckdb_db_graphdb.py)

The `duckdb_db_graphdb.py` script is used to initialize the database environment, creating or recreating the DuckDB databases for each dataset and populating them with initial data.

**Operational Details:**

* **Path Setup:** Sets essential paths (`data`, `project.duckdb`, etc.).
* **Dataset Scan:** Scans all subfolders inside `../data` (e.g., `geo`, `movies`, `world`, etc.), where each folder represents a dataset.
* **For Each Dataset:**
    1.  **Cleanup:** If it exists, deletes the old database (`dataset_name.duckdb`).
    2.  **Creation:** Creates a new, empty DuckDB database.
    3.  **SQL Script Execution:** Automatically runs all ingestion SQL scripts (`ingest_*.sql`) within that dataset folder (table creation, data loading, etc.).
    4.  **Verification:** Displays a list of the newly created tables for verification.
    5.  **Closure:** Closes the connection and persists the database state.

---

### 3.  Query Execution and Result Saving (JSON)

Following the database setup, the `run_queries_to_json.py` script manages the automatic execution of analysis queries and the saving of results for verification.

**Operational Details (`run_queries_to_json.py`):**

* **Target:** The script targets a specific dataset folder (e.g., `../data/geo/`).
* **Automatic Execution:** It automatically executes all SQL queries defined in files matching the pattern `queries_*.sql` within the selected dataset folder.
* **Output:** It saves the result of each executed SQL query into a dedicated **JSON** file.
* **Save Path:** The JSON files are deposited in the path: `../../verification-test-tools/tests/<dataset_name>/`.
* **Function:** This process is crucial for generating the reference results needed for testing and data verification.

---

### 4. Logical and Physical Plan Extraction (EXPLAIN & EXPLAIN ANALYZE)

After generating the query results, the next step is to automatically extract both the **logical** and **physical** query plans for all datasets.

#### Scripts Involved:
* `src/db/run_explain_plans.py`
* `src/db/duckdb_explain.py`


#### **Operational Details (`run_explain_plans.py`):**

1. **Dataset Iteration:**  
   Iterates through each dataset folder inside `data/` (e.g., `world`, `geo`, `flight-2`, `movies`, etc.).

2. **Query Extraction:**  
   Automatically reads all `.sql` files (e.g., `queries_world.sql`), splitting them into individual SQL statement.

3. **Plan Generation:**  
   For each query, connects to the corresponding `.duckdb` database and executes:
 * **`EXPLAIN`**: Produces the **logical plan** showing the operator tree  
  (e.g., `Projection`, `Filter`, `TableScan`, `Join`, `OrderBy`, etc.)
 * **`EXPLAIN ANALYZE`**: Executes the query and reports **profiling data** such as row counts, execution time, and cost estimation per operator.

4. **Saving the Output:**  
   The function `save_both()` (in `duckdb_explain.py`) automatically stores both outputs in separate folders under the projectâ€™s`results/` directory:

   
   Producing four files for each query:
   `<query_name>__explain.txt`
   `<query_name>__explain.json`
   `<query_name>__analyze.txt`
   `<query_name>__analyze.json`

5. **Text & JSON Representation:**
   **.txt** for human-readable format with ASCII boxes (logical/physical plan)
   **.jso**  for machine-readable structured format (escaped Unicode characters)

---

## Avg. Expected Cells Metric 
Like in the GALOIS paper, we replicate the Table 2 of the paper, the **Avg_Expected_Cells** metric is calculated by  **`avg_cells_metric.py`**, for run it you have to locate in the root directory **`galileo_proj/`** directory and next run: **`python -m src.db.avg_cells_metric`**.