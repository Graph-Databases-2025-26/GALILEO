\documentclass[12pt,a4paper]{article}

% Useful packages
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}


% Margini
\usepackage[margin=2.5cm]{geometry}

% Document Informations
\title{31 october 2025 Deadline Report}
\author{Francesco Pivotto 2158296\\ Giorgia Amato 2159999\\ Alessio Demo 2142885 }
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction }
In this part of our project, we deal with the database containing all the data that we need for our purpose.
In details we need to establish a connection to connect the database with the data resources, that allows us to create an "running instance" of the database which we can use to query the database itself and store
the results returned, in particular in our case the results will be stored in a custom JSON format called 'Simple JSON format' provided by the tutor.
Next we will carry out the evaluation of the results  produced by our system using the ground truth data as comparator.\\
Let's go into details.

\section{Repository Structure}
The Structure of out project is organized in scopes (data resources that include database data - ground truth data and output results, configuration stuff,  source scripts, tests), so we have:\\
\begin{itemize}
    \item \textbf{data} folder: Contains all the datasets with the ingest, queries and table creation files, in this folder we store also the ground truth data and the results delivered by our system.
    \item \textbf{config} folder: All the scripts and config files for the initialization of the system are there.
    \item \textbf{src} folder: Here we have the scripts that develop the core functionalities of our system and also other useful utils. 
    \item \textbf{test} folder: Finally this folder contains the unit tests that check the system correctness.
\end{itemize}

\section{Implementation Details}

\subsection{Database-Dataset Connection and Management}
For our purpose we will use DuckDB as database manager, we believe that it is the ideal for our project since it is very powerful and rapid in the data processing within the system, moreover duckDB is an embedded database manager and can run within the instance of our system instead of establishing a server connection like postgres.
At more technical level the \textbf{db\_connection.py} connects to a DuckDB database file associated with a specific dataset.
The next step is \textbf{duckdb\_db\_graphdb.py} which allows us to automatically build DuckDB databases from a directory of datasets, each containing an SQL ingestion script that creates the data tables and populates them,  this allows us to have an instance of the db representing a particular dataset  in which we can run queries.

\subsection{Query Execution}
The queries can be execute by means \textbf{run\_queries\_to\_json.py} script that automates the execution of SQL queries on DuckDB databases and exports their results to JSON files, one per query.\\
Going more deep the script executes each query and saves the results as JSON:

\begin{enumerate}
    \item Iterates through each query tuple.
    \item Executes the SQL query using the provided DuckDB connection.
    \item Fetches the resulting rows and column names.
    \item Converts the result into a list of dictionaries.
    \item Saves each query result as a JSON file in the output directory.
\end{enumerate}

\subsubsection{Explain Plans (DuckDB)}
\label{sec:explain-plans}

The \texttt{EXPLAIN} module was implemented to validate the SQL execution
pipeline on DuckDB and to visualise the difference between logical and
physical plans, providing a reliable baseline for the comparison with the
GALOIS framework. 

\paragraph{Implementation Overview}
The functionality is implemented in the main script
\path{src/db/run_explain_plans.py}, which delegates the
actual DuckDB interaction to the adapter
\path{src/db/duckdb\_explain.py}. The script can be executed by passing
either a specific dataset name (e.g., \texttt{world}) or the keyword
\texttt{all}. For each dataset located in
\path{<repo-root>/data/<dataset>}, the corresponding database
\texttt{<dataset>.duckdb} and all \texttt{queries\_*.sql} files are loaded,
split into single statements, and both \texttt{EXPLAIN} and
\texttt{EXPLAIN ANALYZE} are executed.

The module’s core responsibilities are:
\begin{enumerate}
  \item \textbf{Dataset iteration:} enumerates datasets and SQL files and assigns a stable query identifier
        (e.g., \path{queries\_world\_\_q3});
  \item \textbf{Parsing:} splits multi-statement SQL files on semicolons while preserving comments for traceability;
  \item \textbf{Execution:} connects to the dataset’s \texttt{.duckdb} database and issues both \texttt{EXPLAIN} and
        \texttt{EXPLAIN ANALYZE};
  \item \textbf{Persistence:} stores all outputs through the \texttt{save\_both()} function.
\end{enumerate}

\paragraph{Output Structure}
The results are written under the global \texttt{results/} directory in two
parallel trees:
\begin{itemize}
  \item \path{explain_result/} for text files (\texttt{.txt}) for human inspection;
  \item \path{explain_result_json/} equivalent JSON files for machine processing.
\end{itemize}

Each of these folders contains one subdirectory per dataset
(\texttt{flight-2}, \texttt{flight-4}, \texttt{fortune}, \texttt{geo},
\texttt{premier}, \texttt{presidents}, \texttt{world}). For every SQL statement,
four output files are produced sharing the same base name (e.g.,
\texttt{queries\_world\_\_q3}):
\begin{itemize}
  \item \texttt{\_\_explain.txt} and \texttt{\_\_explain.json} for logical and physical plans;
  \item \texttt{\_\_analyze.txt} and \texttt{\_\_analyze.json} for runtime statistics and operator timings.
\end{itemize}

\paragraph{Example}
A representative example from the \texttt{world} dataset is:
\begin{verbatim}
SELECT count(DISTINCT government_form)
FROM target.country
WHERE continent = 'Africa';
\end{verbatim}

This query counts the number of distinct government forms for African countries.
The generated logical plan follows a typical structure
(\emph{Aggregate} $\rightarrow$ \emph{Filter} $\rightarrow$ \emph{TableScan}),
while the physical plan contains the corresponding operators
(\emph{HASH\_AGGREGATE} $\rightarrow$ \emph{FILTER} $\rightarrow$
\emph{SEQ\_SCAN}). The analyzed output reports the number of processed rows and
the execution time per operator.

\paragraph{Role in the Project}
This component ensures that the database layer behaves as expected and that
query plans can be reliably generated and compared. The outputs will later be
used to align relational operators with their LLM-based equivalents within the
GALOIS pipeline.


\subsection{Evaluation}
We have the \textbf{galois\_eval.py} provided by Tutor, that is a command-line evaluator that computes performance metrics for query results across different datasets.
Briefly it standardizes different result formats, normalizes textual/numeric data, computes multiple precision–recall-based metrics, aggregates them across datasets, and outputs results in several human- or machine-readable formats.

\subsection{Statistics for the datasets in the experiments}
Table~\ref{tab:datasets} provides an overview of the datasets used our project. The datasets are divided into two main categories based on the type of task: IK (Incomplete Knowledge) and MC (Multiple Choice).
The MC-type datasets include PREMIER, sourced from BBC, and FORTUNE, based on Kaggle data.
Finally, GEO-TEST represents the test dataset, also derived from Spider [68], used to validate the model's performance.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Dataset} & \textbf{\# of} & \textbf{Avg. expected} & \textbf{Type} \\
\textbf{name} & \textbf{source} & \textbf{queries} & \textbf{cells} & \\
\midrule
FLIGHT2 & Spider [68] & 3 & 267.5 & IK \\
FLIGHT4 & Spider [68] & 3 & 267.5 & IK \\
FORTUNE & Kaggle & 10 & 7.9 & MC \\
GEO & Spider [68] & 32 & 22.8 & IK \\
MOVIES & IMDB & 9 & 54.7 & IK \\
PREMIER & BBC & 5 & 57.8 & MC \\
PRESIDENTS & Wiki & 26 & 42.2 & IK \\
WORLD & Spider [68] & 4 & 33.2 & IK \\



\midrule
GEO-TEST & Spider [68] & 10 & 24.1 & IK \\
\bottomrule
\end{tabular}
\caption{Description of used datasets}
\label{tab:datasets}
\end{table}



\end{document}