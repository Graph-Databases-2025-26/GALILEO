\documentclass[12pt,a4paper]{article}

% Useful packages
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}


% Margini
\usepackage[margin=2.5cm]{geometry}

% Document Informations
\title{31 october 2025 Deadline Report}
\author{Francesco Pivotto 2158296\\ Giorgia Amato 2159999\\ Alessio Demo 2142885 }
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction }
In this part of our project, we deal with the database containing all the data that we need for our purpose.
In details we need to establish a connection to connect the database with the data resources, that allows us to create an "running instance" of the database which we can use to query the database itself and store
the results returned, in particular in our case the results will be stored in a custom JSON format called 'Simple JSON format' provided by the tutor.
Next we will carry out the evaluation of the results  produced by our system using the ground truth data as comparator.\\
Let's go into details.

\section{Libraries \& Components}
\subsection{System Architecture: Dependency Stack and Key Requirements:}
The project is built upon a Python dependency stack prioritizing robustness, efficient data analysis, and flexible configuration. All essential libraries are listed in requirements.txt.
\begin{enumerate}
  \item Core Frameworks \& Data Analysis (DataFrames):
  \textbf{pandas} and \textbf{numpy} are used for the efficient manipulation and analysis of tabular data structures (DataFrames) and complex numerical calculations.
  \item Database and SQL:
  \textbf{duckdb}: Utilized as a high-performance, in-process analytical database, ideal for rapid processing of data volumes directly within the application.
  \textbf{sqlglot}: Employed for the abstract analysis, manipulation, and translation of SQL queries.
  \item LLM Orchestrator (Large Language Model)
  \textbf{LangChain}: The core framework managing interactions with AI models.
  \textbf{langchain-google-genai}: Provides access to Google Gemini AI models. This setup ensures modularity, flexibility, and scalability in establishing the system's communication with the LLM.
  \item  Configuration and Data Validation:
  \textbf{pydantic}: Essential for data validation and structured configuration management. It enforces type safety, handles data parsing/coercion, defines structured nested models, and raises immediate errors on malformed input.
  \textbf{python-dotenv}: Loads environment variables (including secrets) from the .env file.
  \textbf{pyyaml}: Handles reading and writing configuration files in YAML format.
  \item  Templating: \textbf{jinja2}: The templating engine used for the dynamic generation of structured output, such as code, reports (HTML, XML, JSON), or complex prompt statements, by separating presentation logic from data.
  \item Logging and Utilities
  \textbf{loguru}: The chosen logging library for its simplicity and structured output, which enhances system log readability.
  Logs: All runtime logs are stored in the .log folder.
  \item Development and Testing
  These dependencies are specific to the development environment and the test suite:
  \textbf{pytest}: The standard testing framework for executing all tests.
  coverage and \textbf{pytest-cov}: Used to measure the percentage of source code covered by automated tests (Code Coverage).
  \textbf{pytest-mock}: Facilitates the creation of mock objects and stubs to isolate external dependencies during unit tests.


\end{enumerate}

\section{Repository Structure}
The Structure of out project is organized in scopes (data resources that include database data - ground truth data and output results, configuration stuff,  source scripts, tests), so we have:\\
\begin{itemize}
    \item \textbf{data} folder: Contains all the datasets with the ingest, queries and table creation files, in this folder we store also the ground truth data and the results delivered by our system.
    \item \textbf{config} folder: All the scripts and config files for the initialization of the system are there.
    \item \textbf{src} folder: Here we have the scripts that develop the core functionalities of our system and also other useful utils. 
    \item \textbf{test} folder: Finally this folder contains the unit tests that check the system correctness.
\end{itemize}

\section{Implementation Details}

\subsection{How the project can be launched}
To get started, you just need to launch the main setup script (\textbf{galileo\_proj/setup\_project.py}). We’ve designed it to be flexible, so you can run it with or without specifying datasets — either directly from the command line or through a configuration file.
\subsubsection{Basic Execution}
If you simply want to run the full setup without worrying about parameters, just type: \textbf{python setup\_project.py all}, this will automatically process all available datasets.
\subsubsection{Running with Specific Datasets}
If you want to focus on one or more specific datasets, you can pass them as arguments when launching the script. For example: \textbf{python setup\_project.py GEO MOVIES}, this will automatically process all available datasets.
\subsubsection{Using the YAML Configuration}
If you don’t pass any arguments from the command line, the script will automatically fall back to this configuration.
You can define the datasets in the configuration file located at: \textbf{config/config.yaml}, just edit the \textbf{database} attribute
\subsubsection{Default Behavior}
No arguments? No YAML entry? No problem. The script will default to processing all datasets, so you’re always covered.

\subsection{Configuration handling}
All the system configuration details are reported  in \textbf{.yaml} file, and the \textbf{src/settings.py} is the central hub for all configuration data. It handles the critical process of loading and validating the external YAML file, ensuring the application always runs with correct and predictable parameters.
Going more deeply:
\begin{itemize}
  \item Parsing (YAML): The script uses \textbf{pyyaml} to read and parse the content of the config.yaml file into a standard Python dictionary.
  \item Structured Validation (Pydantic): The parsed data is immediately processed and validated using Pydantic.
    \begin{itemize}
      \item   Structure: We define precise, nested classes (like ModelConfig, IOConfig, ExecutionConfig, etc.) that mirror the expected structure of the YAML file. The main class, AppConfig, aggregates all these sections.
      \item Type\ Safety: Pydantic strictly enforces data types (e.g., ensuring temperature is a float, and max\_retries is an int). This eliminates common runtime errors caused by misconfigured settings.
      \item Predictability: By validating the structure and types upfront, we guarantee that all required fields are present and correctly formatted, making the configuration predictable and reliable.
    \end{itemize}  
  \item Global Access: The configuration is loaded into a single, global instance called settings. This allows any module in the project to access any configuration detail easily and consistently, such as settings.gemini.temperature.
\end{itemize}

\subsection{Database-Dataset Connection and Management}
For our purpose we will use DuckDB as database manager, we believe that it is the ideal for our project since it is very powerful and rapid in the data processing within the system, moreover duckDB is an embedded database manager and can run within the instance of our system instead of establishing a server connection like postgres.
At more technical level the \textbf{db\_connection.py} connects to a DuckDB database file associated with a specific dataset.
The next step is \textbf{duckdb\_db\_graphdb.py} which allows us to automatically build DuckDB databases from a directory of datasets, each containing an SQL ingestion script that creates the data tables and populates them,  this allows us to have an instance of the db representing a particular dataset  in which we can run queries.

\subsection{Query Execution}
The queries can be execute by means \textbf{run\_queries\_to\_json.py} script that automates the execution of SQL queries on DuckDB databases and exports their results to JSON files, one per query.\\
Going more deep the script executes each query and saves the results as JSON:

\begin{enumerate}
    \item Iterates through each query tuple.
    \item Executes the SQL query using the provided DuckDB connection.
    \item Fetches the resulting rows and column names.
    \item Converts the result into a list of dictionaries.
    \item Saves each query result as a JSON file in the output directory.
\end{enumerate}

\subsubsection{Explain Plans (DuckDB)}
\label{sec:explain-plans}

The \texttt{EXPLAIN} module was implemented to validate the SQL execution
pipeline on DuckDB and to visualise the difference between logical and
physical plans, providing a reliable baseline for the comparison with the
GALOIS framework. 

\paragraph{Implementation Overview}
The functionality is implemented in the main script
\path{src/db/run_explain_plans.py}, which delegates the
actual DuckDB interaction to the adapter
\path{src/db/duckdb\_explain.py}. The script can be executed by passing
either a specific dataset name (e.g., \texttt{world}) or the keyword
\texttt{all}. For each dataset located in
\path{<repo-root>/data/<dataset>}, the corresponding database
\texttt{<dataset>.duckdb} and all \texttt{queries\_*.sql} files are loaded,
split into single statements, and both \texttt{EXPLAIN} and
\texttt{EXPLAIN ANALYZE} are executed.

The module’s core responsibilities are:
\begin{enumerate}
  \item \textbf{Dataset iteration:} enumerates datasets and SQL files and assigns a stable query identifier
        (e.g., \path{queries\_world\_\_q3});
  \item \textbf{Parsing:} splits multi-statement SQL files on semicolons while preserving comments for traceability;
  \item \textbf{Execution:} connects to the dataset’s \texttt{.duckdb} database and issues both \texttt{EXPLAIN} and
        \texttt{EXPLAIN ANALYZE};
  \item \textbf{Persistence:} stores all outputs through the \texttt{save\_both()} function.
\end{enumerate}

\paragraph{Output Structure}
The results are written under the global \texttt{results/} directory in two
parallel trees:
\begin{itemize}
  \item \path{explain_result/} for text files (\texttt{.txt}) for human inspection;
  \item \path{explain_result_json/} equivalent JSON files for machine processing.
\end{itemize}

Each of these folders contains one subdirectory per dataset
(\texttt{flight-2}, \texttt{flight-4}, \texttt{fortune}, \texttt{geo},
\texttt{premier}, \texttt{presidents}, \texttt{world}). For every SQL statement,
four output files are produced sharing the same base name (e.g.,
\texttt{queries\_world\_\_q3}):
\begin{itemize}
  \item \texttt{\_\_explain.txt} and \texttt{\_\_explain.json} for logical and physical plans;
  \item \texttt{\_\_analyze.txt} and \texttt{\_\_analyze.json} for runtime statistics and operator timings.
\end{itemize}

\paragraph{Example}
A representative example from the \texttt{world} dataset is:
\begin{verbatim}
SELECT count(DISTINCT government_form)
FROM target.country
WHERE continent = 'Africa';
\end{verbatim}

This query counts the number of distinct government forms for African countries.
The generated logical plan follows a typical structure
(\emph{Aggregate} $\rightarrow$ \emph{Filter} $\rightarrow$ \emph{TableScan}),
while the physical plan contains the corresponding operators
(\emph{HASH\_AGGREGATE} $\rightarrow$ \emph{FILTER} $\rightarrow$
\emph{SEQ\_SCAN}). The analyzed output reports the number of processed rows and
the execution time per operator.

\paragraph{Role in the Project}
This component ensures that the database layer behaves as expected and that
query plans can be reliably generated and compared. The outputs will later be
used to align relational operators with their LLM-based equivalents within the
GALOIS pipeline.


\subsection{Evaluation}
We have the \textbf{galois\_eval.py} provided by Tutor, that is a command-line evaluator that computes performance metrics for query results across different datasets.
Briefly it standardizes different result formats, normalizes textual/numeric data, computes multiple precision–recall-based metrics, aggregates them across datasets, and outputs results in several human- or machine-readable formats.

\subsection{Statistics for the datasets in the experiments}
Table~\ref{tab:datasets} provides an overview of the datasets used our project. The datasets are divided into two main categories based on the type of task: IK (Incomplete Knowledge) and MC (Multiple Choice).
The MC-type datasets include PREMIER, sourced from BBC, and FORTUNE, based on Kaggle data.
Finally, GEO-TEST represents the test dataset, also derived from Spider [68], used to validate the model's performance.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Dataset} & \textbf{\# of} & \textbf{Avg. expected} & \textbf{Type} \\
\textbf{name} & \textbf{source} & \textbf{queries} & \textbf{cells} & \\
\midrule
FLIGHT2 & Spider [68] & 3 & 1.0 & IK \\
FLIGHT4 & Spider [68] & 3 & 534.0 & IK \\
FORTUNE & Kaggle & 10 & 11.1 & MC \\
GEO & Spider [68] & 32 & 23.1 & IK \\
MOVIES & IMDB & 9 & 50.9 & IK \\
PREMIER & BBC & 5 & 57.4 & MC \\
PRESIDENTS & Wiki & 26 & 41.2 & IK \\
WORLD & Spider [68] & 4 & 33.2 & IK \\


\bottomrule
\end{tabular}
\caption{Description of used datasets}
\label{tab:datasets}
\end{table}


For the \textbf{Avg\_expected\_cells} parameter we have implemented an ad hoc script that calculates it, more technically the \textbf{avg\_cells\_metric.py} calculates for each dataset the sum of the cells returned by the results of each query and then adds them together.
Finally, it calculates the metric by dividing the total number of cells by the number of queries for that specific dataset.




\end{document}