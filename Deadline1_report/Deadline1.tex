\documentclass[12pt,a4paper]{article}

% Useful packages
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}
\usepackage{float}
\usepackage{booktabs}
\usepackage{setspace} 
\usepackage{float} 

% Margini
\usepackage[margin=2.5cm]{geometry}

% Document Informations
\title{31 october 2025 Deadline Report}
\author{Francesco Pivotto 2158296\\ Giorgia Amato 2159999\\ Alessio Demo 2142885 }
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction }
In this part of our project, we deal with the database containing all the data that we need for our purpose.
In details we need to establish a connection to connect the database with the data resources, that allows us to create an "running instance" of the database which we can use to query the database itself and store
the results returned, in particular in our case the results will be stored in a custom JSON format called 'Simple JSON format' provided by the tutor.
Next we will carry out the evaluation of the results  produced by our system using the ground truth data as comparator.\\
Let's go into details.

\section{Libraries \& Components}
\subsection{System Architecture: Dependency Stack and Key Requirements:}
The project is built upon a Python dependency stack prioritizing robustness, efficient data analysis, and flexible configuration. All essential libraries are listed in requirements.txt.
\begin{enumerate}
  \item Core Frameworks \& Data Analysis (DataFrames):
  \textbf{pandas} and \textbf{numpy} are used for the efficient manipulation and analysis of tabular data structures (DataFrames) and complex numerical calculations.
  \item Database and SQL:
  \textbf{duckdb}: Utilized as a high-performance, in-process analytical database, ideal for rapid processing of data volumes directly within the application.
  \textbf{sqlglot}: Employed for the abstract analysis, manipulation, and translation of SQL queries.
  \item LLM Orchestrator (Large Language Model)
  We have implemented a streamlined pipeline to interact with Google’s Gemini Large Language Model (LLM) using the \textbf{LangChain} framework and with IBM watsonx ai using the \textbf{ModelInference} package from  \textbf{ibm\_watsonx\_ai.foundation\_models}.
  \\\textbf{LangChain}: The core framework managing interactions with the AI model.
  \\Going deeper in the technical stuff fro Google Gemini LLM:
  \begin{itemize}
    \item  API Key Management Securely loads API keys from environment variables using dotenv, ensuring flexible and safe configuration.
    \item  LLM Initialization Instantiates a ChatGoogleGenerativeAI object with customizable parameters such as model version (gemini-2.5-flash) and temperature.
    \item  Model Warm-Up Performs a quick direct prompt to initialize the model and measure response latency.
    \item  LangChain Prompt Chaining Uses a structured prompt template in Italian to generate clear and concise answers. The chain includes:
        \begin{itemize}
          \item  ChatPromptTemplate: dynamically formats the prompt
          \item  ChatGoogleGenerativeAI: generates the response
          \item  StrOutputParser: extracts the textual output
        \end{itemize}
  \end{itemize}
  Then regarding IBM watsonx ai LLM:
  \begin{itemize}
    \item  API Key Management Securely loads API keys from environment variables using dotenv, ensuring flexible and safe configuration.
    \item  The code builds a ModelInference object configured with the credentials, project ID, and chosen model.
    \item  The system submits the prompt and receives the response from the LLM.
    \item  Logging the prompt, response length, latency, and any errors (including Python stack traces if exceptions occur)
  \end{itemize}
  In summary he script sends a sample prompt and logs the LLM’s output.

  \item  Configuration and Data Validation:
  \textbf{pydantic}: Essential for data validation and structured configuration management. It enforces type safety, handles data parsing/coercion, defines structured nested models, and raises immediate errors on malformed input.
  \textbf{python-dotenv}: Loads environment variables (including secrets) from the .env file.
  \textbf{pyyaml}: Handles reading and writing configuration files in YAML format.
  \item  Templating: \textbf{jinja2}: The templating engine that will be used for the dynamic generation of structured output, such as code, reports (HTML, XML, JSON), or complex prompt statements, by separating presentation logic from data.
  \item Logging and Utilities
  \textbf{loguru}: We implement structured logging with \texttt{loguru} instead of the basic Python \texttt{logging} module. 
The goal is to have uniform, timestamped messages with contextual metadata (dataset, query, latency, token usage), visible in the console and saved to rotating log files.
Specifically, when the code is executed, a “logs” folder will be generated and inside it there will be a text file containing all the system logs.
\begin{itemize}
  \item \textbf{Structured and unified output:} provides a single, centralized configuration for message format, log levels, and output handlers.
  \item \textbf{Enhanced observability:} captures execution timings (in milliseconds) for DuckDB \texttt{EXPLAIN}/\texttt{EXPLAIN ANALYZE} operations and LLM requests, including token usage statistics when available.
  \item \textbf{Data persistence and maintenance:} ensures automatic log file rotation, retention, and compression to manage disk usage and prevent oversized log artifacts.
\end{itemize}
We will see later in the report more details about the Logs handling.
  \item Development and Testing
  \textbf{pytest}: The standard testing framework for executing all tests.
  coverage and \textbf{pytest-cov}: Used to measure the percentage of source code covered by automated tests (Code Coverage).
  \textbf{pytest-mock}: Facilitates the creation of mock objects and stubs to isolate external dependencies during unit tests.


\end{enumerate}



\section{Project Structure}

The project follows a modular organization designed to separate configuration, data, source code, and testing components.  
This layout enhances maintainability, scalability, and code clarity.

Before seeing the project's structure, we want to highlight the different design patterns used: 
The \textbf{src} folder is divided into logical packages (db, llm, utils, test, etc.), that highlights the separation of concerns, provides a easier navigation and a modular code organization.
Moreover within the \textbf{src/llm} there are different files that interfaces between the systems and various language model providers, this behavior can be seen as an  \textbf{Adapter Pattern}.\\
In our project we applied also  a \textbf{Repository Pattern} by means a single script for connecting the database e instantiating it within each dataset folder.
Finally the main script provides a single interface that hides the complexity and the logic of underlying functionalities (db connection, sql queries handling, LLM interaction ...). 

\subsection{Main Directories and Contents}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{2.0}
\begin{tabular}{p{0.18\linewidth} p{0.75\linewidth}}
\toprule
\textbf{Directory} & \textbf{Description and Contents} \\ 
\midrule

\textbf{/config} &
Contains configuration files and the logic for their loading.  
Includes:\vspace{-10pt}
\begin{flushleft}
\setstretch{1}
\textbf{config.yaml} — Main configuration file.\\[1pt]
\textbf{loaders.py} — Parses the YAML configuration into a Python structure.
\end{flushleft}
\\

\textbf{/data} &
Stores raw data, ground truth, and intermediate results.  
Contains:\vspace{-10pt}
\begin{flushleft}
\setstretch{1}
\textbf{/.ground\_truth} — Baseline truth for evaluation.\\[2pt]
\textbf{/.output} — Generated results ready for comparison.
\end{flushleft}
\\

\textbf{/results} &
Holds the final outputs produced by the analysis process.  
Typically includes \texttt{.json} and \texttt{.txt} files from the \texttt{explain} and \texttt{analyze} operations.
\\

\textbf{/src} &
Main source folder containing the project’s operational logic.  
Includes:\vspace{-10pt}
\begin{flushleft}
\setstretch{1}
\textbf{main.py} — Entry point for execution.\\[2pt]
\textbf{/db} — Database interaction modules (e.g., \texttt{db\_connection.py}, \texttt{run\_queries\_to\_json.py}).\\[2pt]
\textbf{/llm} — Interfaces with Large Language Models (e.g., \texttt{google\_genai\_connection.py}, \texttt{openai\_connection.py}).\\[2pt]
\textbf{/galois} — Implements query optimization and execution logic inspired by the Galois architecture.\\[2pt]
\textbf{/utils} — Utility scripts: \texttt{constants.py}, \texttt{logging\_config.py}, \texttt{build\_ground\_truth.py}, \texttt{galois\_eval.py}.
\end{flushleft}
\\

\textbf{/test} &
Contains unit and integration testing scripts (e.g., \texttt{db\_utils.py}).
\\

\bottomrule
\end{tabular}
\caption{Main directories and their contents.}
\end{table}

\subsection{Root-Level Files}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{p{0.25\linewidth} p{0.65\linewidth}}
\toprule
\textbf{File} & \textbf{Description} \\
\midrule
\textbf{README.md} & High-level project documentation. \\
\textbf{requirements.txt} & List of required Python dependencies. \\
\textbf{setup\_project.py} & Script for project initialization. \\
\bottomrule
\end{tabular}
\caption{Root-level files.}
\end{table}


\section{Implementation Details}

\subsection{How the project can be launched}
To get started, the only scripts that you need is \textbf{main.py} and \textbf{setup\_project.py} that is located in the root folder.
\\In detail:
\textbf{setup\_project.py} install all the needed libraries for requirements.txt and creates the virtual environment;
\textbf{main.py} runs all the system functionalities. We’ve designed it to be flexible, so you can run it with or without specifying datasets — either directly from the command line or through the YAML configuration file.
\\\textbf{ATTENTION}: If is the first time that you run the system, before running \textbf{main.py} you must run \textbf{setup\_project.py} for creating the virtual environment and the needed python libraries.
You can run these scripts either with a single command  (in linux you can use '\&\&' to concatenate the two scripts and the same for command prompt in windows, in other hand if you are using bash prompt you need to use ';') or individually, both options work just fine.
\subsubsection{Basic Execution}
If you simply want to run the full setup without worrying about parameters, just type: \textbf{python setup\_project.py all}, this will automatically process all available datasets.
\subsubsection{Running with Specific Datasets}
If you want to focus on one or more specific datasets, you can pass them as arguments when launching the scripts. For example: \textbf{python setup\_project.py \&\&  python main.py world flight\-2 presidents}.
\subsubsection{Using the YAML Configuration}
If you don’t pass any arguments from the command line, the script will automatically fall back to this configuration.
You can define the datasets in the configuration file located at: \textbf{config/config.yaml}, just need to edit the \textbf{database} attribute.
\subsubsection{Default Behavior}
No arguments? No YAML entry? No problem. The script will default to processing all datasets, so you’re always covered.

\subsection{Configuration handling}
All the system configuration details are reported  in \textbf{.yaml} file, and the \textbf{src/settings.py} is the central hub for all configuration data. It handles the critical process of loading and validating the external YAML file, ensuring the application always runs with correct and predictable parameters.
Going more deeply:
\begin{itemize}
  \item Parsing (YAML): The script uses \textbf{pyyaml} to read and parse the content of the config.yaml file into a standard Python dictionary.
  \item Structured Validation (Pydantic): The parsed data is immediately processed and validated using Pydantic.
    \begin{itemize}
      \item   Structure: We define precise, nested classes (like ModelConfig, IOConfig, ExecutionConfig, etc.) that mirror the expected structure of the YAML file. The main class, AppConfig, aggregates all these sections.
      \item Type\ Safety: Pydantic strictly enforces data types (e.g., ensuring temperature is a float, and max\_retries is an int). This eliminates common runtime errors caused by misconfigured settings.
      \item Predictability: By validating the structure and types upfront, we guarantee that all required fields are present and correctly formatted, making the configuration predictable and reliable.
    \end{itemize}  
  \item Global Access: The configuration is loaded into a single, global instance called settings. This allows any module in the project to access any configuration detail easily and consistently, such as settings.gemini.temperature.
\end{itemize}

\subsection{Database-Dataset Connection and Management}
For our purpose we will use DuckDB as database manager, we believe that it is the ideal for our project since it is very powerful and rapid in the data processing within the system, moreover duckDB is an embedded database manager and can run within the instance of our system instead of establishing a server connection like postgres.
At more technical level the \textbf{db\_connection.py} connects to a DuckDB database file associated with a specific dataset.
The next step is \textbf{duckdb\_db\_graphdb.py} which allows us to automatically build DuckDB databases from a directory of datasets, each containing an SQL ingestion script that creates the data tables and populates them,  this allows us to have an instance of the db representing a particular dataset  in which we can run queries.

\subsection{Query Execution}
The queries can be execute by means \textbf{run\_queries\_to\_json.py} script that automates the execution of SQL queries on DuckDB database and exports their results to JSON files, one per query.\\
Going more deep the script executes each query and saves the results as JSON:

\begin{enumerate}
    \item Iterates through each query tuple.
    \item Executes the SQL query using the provided DuckDB connection.
    \item Fetches the resulting rows and column names.
    \item Converts the result into a list of dictionaries.
    \item Saves each query result as a JSON file in the output directory.
\end{enumerate}

\subsubsection{Explain Plans (DuckDB)}
\label{sec:explain-plans}

The \texttt{EXPLAIN} module was implemented to validate the SQL execution
pipeline on DuckDB and to visualize the difference between logical and
physical plans, providing a reliable baseline for the comparison with the
GALOIS framework. 

\paragraph{Implementation Overview}
The functionality is implemented in the main script
\path{src/db/run_explain_plans.py}, which delegates the
actual DuckDB interaction to the adapter
\path{src/db/duckdb\_explain.py}. The script can be executed by passing
one or more dataset names (e.g., \texttt{world}) or the keyword
\texttt{all} to process them all together. For each dataset located in
\path{<repo-root>/data/<dataset>}, the corresponding database
\texttt{<dataset>.duckdb} and all \texttt{queries\_*.sql} files are loaded,
split into single statements, and both \texttt{EXPLAIN} and
\texttt{EXPLAIN ANALYZE} are executed.

The module’s core responsibilities are:
\begin{enumerate}
  \item \textbf{Dataset iteration:} enumerates datasets and SQL files and assigns a stable query identifier
        (e.g., \path{queries\_world\_\_q3});
  \item \textbf{Parsing:} splits multi-statement SQL files on semicolons while preserving comments for traceability;
  \item \textbf{Execution:} connects to the dataset’s \texttt{.duckdb} database and issues both \texttt{EXPLAIN} and
        \texttt{EXPLAIN ANALYZE};
  \item \textbf{Persistence:} stores all outputs through the \texttt{save\_both()} function.
\end{enumerate}

\paragraph{Output Structure}
The results are written under the global \texttt{results/} directory in two
parallel trees:
\begin{itemize}
  \item \path{explain_result/} for text files (\texttt{.txt}) for human inspection;
  \item \path{explain_result_json/} equivalent JSON files for machine processing.
\end{itemize}

Each of these folders contains one subdirectory per dataset
(\texttt{flight-2}, \texttt{flight-4}, \texttt{fortune}, \texttt{geo},
\texttt{premier}, \texttt{presidents}, \texttt{world}). For every SQL statement,
four output files are produced sharing the same base name (e.g.,
\texttt{queries\_world\_\_q3}):
\begin{itemize}
  \item \texttt{\_\_explain.txt} and \texttt{\_\_explain.json} for logical and physical plans;
  \item \texttt{\_\_analyze.txt} and \texttt{\_\_analyze.json} for runtime statistics and operator timings.
\end{itemize}

\paragraph{Example}
A representative example from the \texttt{world} dataset is:
\begin{verbatim}
SELECT count(DISTINCT government_form)
FROM target.country
WHERE continent = 'Africa';
\end{verbatim}

This query counts the number of distinct government forms for African countries.
The generated logical plan follows a typical structure
(\emph{Aggregate} $\rightarrow$ \emph{Filter} $\rightarrow$ \emph{TableScan}),
while the physical plan contains the corresponding operators
(\emph{HASH\_AGGREGATE} $\rightarrow$ \emph{FILTER} $\rightarrow$
\emph{SEQ\_SCAN}). The analyzed output reports the number of processed rows and
the execution time per operator.

\paragraph{Role in the Project}
This component ensures that the database layer behaves as expected and that
query plans can be reliably generated and compared. The outputs will later be
used to align relational operators with their LLM-based equivalents within the
GALOIS pipeline.

\subsection{Logging \& Observability}
\label{subsec:logging-impl}

\paragraph{Configuration.}
Logging is centralized in \texttt{src/utils/logging\_config.py}. It creates a \texttt{logs/} folder at runtime and registers two sinks:
\begin{itemize}
  \item \textbf{Console} (level \texttt{INFO+}, colored): live progress while running.
  \item \textbf{File} \texttt{logs/pipeline.log} (level \texttt{DEBUG+}): persistent, rotated at 5\,MB, retained for 10 days, compressed as \texttt{.zip}.
\end{itemize}
Python warnings are redirected to the logger so that \texttt{UserWarning}/\texttt{DeprecationWarning} also appear in the logs.

\paragraph{How to change verbosity.}
To reduce console noise, change the console sink level to \texttt{WARNING} in
\texttt{logging\_config.py}. The file sink will still persist \texttt{DEBUG+}:
\begin{verbatim}
logger.add(sys.stdout, ..., level="WARNING")
\end{verbatim}

\subsection{Evaluation}
We have the \textbf{galois\_eval.py} provided by Tutor, that is a command-line evaluator that computes performance metrics for query results across different datasets.
Briefly it standardizes different result formats, normalizes textual/numeric data, computes multiple precision–recall-based metrics, aggregates them across datasets, and outputs results in several human- or machine-readable formats.

\section{Results and Statistics}
\subsection{Statistics for the datasets in the experiments}
Table~\ref{tab:datasets} provides an overview of the datasets used our project. The datasets are divided into two main categories based on the type of task: IK (Incomplete Knowledge) and MC (Multiple Choice).
The MC-type datasets include PREMIER, sourced from BBC, and FORTUNE, based on Kaggle data.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Dataset} & \textbf{\# of} & \textbf{Avg. expected} & \textbf{Type} \\
\textbf{name} & \textbf{source} & \textbf{queries} & \textbf{cells} & \\
\midrule
FLIGHT-2 & Spider [68] & 3 & 1.0 & IK \\
FLIGHT-4 & Spider [68] & 3 & 534.0 & IK \\
FORTUNE & Kaggle & 10 & 11.1 & MC \\
GEO & Spider [68] & 32 & 23.1 & IK \\
MOVIES & IMDB & 9 & 50.9 & IK \\
PREMIER & BBC & 5 & 57.4 & MC \\
PRESIDENTS & Wiki & 26 & 41.2 & IK \\
WORLD & Spider [68] & 4 & 33.2 & IK \\


\bottomrule
\end{tabular}
\caption{Description of used datasets}
\label{tab:datasets}
\end{table}


For the \textbf{Avg\_expected\_cells} parameter we have implemented an ad hoc script that calculates it, more technically the \textbf{avg\_cells\_metric.py} calculates for each dataset the sum of the cells returned by the results of each query and then adds them together.
Finally, it calculates the metric by dividing the total number of cells by the number of queries for that specific dataset.
In summary this table  highlights the diversity in dataset complexity and domain coverage, offering a benchmark for evaluating the performance of tabular data querying systems under different conditions, for example: Datasets like FLIGHT-2 involve simple, direct answers.
In other hand the dataset FLIGHT-4  require highly complex and extensive responses.




\end{document}